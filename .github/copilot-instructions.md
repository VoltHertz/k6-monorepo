# GitHub Copilot Instructions - k6 Performance Testing Monorepo

## üéØ Project Overview

This is an **enterprise-grade k6 performance testing monorepo** using TypeScript, designed for 10+ years of maintainability. We test DummyJSON API endpoints with automated quality gates, CI/CD integration, and comprehensive observability.

## üèóÔ∏è Architecture Principles

### ADR-001: TypeScript-First
- k6 runs `.ts` files natively via esbuild (no build step needed: `k6 run test.ts`)
- Run `tsc --noEmit` in CI for type checking only
- Use `@types/k6` for autocomplete/type safety

### ADR-002: Open Model Executors (CRITICAL)
- **Always use `constant-arrival-rate` or `ramping-arrival-rate`** for API tests
- Never use `shared-iterations` or `per-vu-iterations` (closed model) - these don't reflect real user behavior
- Open model creates VUs on-demand to hit target RPS, avoiding VU bottlenecks

### ADR-003: Data Strategy (CRITICAL)
- `data/fulldummyjsondata/` = **READ-ONLY** application dumps (reference only)
- `data/test-data/` = curated test data (CSV/JSON) generated from dumps
- Use `SharedArray` with `open()` to load test data (memory-efficient, thread-safe)

## üìÅ Directory Structure & Conventions

```
tests/api/<feature>/<test-name>.test.ts    # Domain-driven tests (products, users, auth, carts)
tests/scenarios/<journey>.test.ts          # Composite user journeys
libs/http/                                 # HTTP client, checks, interceptors
libs/data/                                 # SharedArray loaders, generators, parsers
libs/scenarios/                            # Executor factories, tag strategies
libs/metrics/                              # Custom Trends/Counters, SLO validators
libs/reporting/                            # handleSummary (JSON/JUnit/HTML)
configs/scenarios/<type>.yaml              # smoke/baseline/stress/soak configs
configs/envs/<env>.json                    # Environment configs (local/ci/prod-like)
docs/casos_de_uso/UC00X-<name>.md         # Use case documentation with SLOs
```

## ‚úÖ Test File Template (MANDATORY STRUCTURE)

```typescript
// tests/api/products/browse-catalog.test.ts
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Trend, Counter } from 'k6/metrics';
import { SharedArray } from 'k6/data';
import { randomItem } from 'https://jslib.k6.io/k6-utils/1.4.0/index.js';
import { getConfig } from '../../../libs/scenarios/profiles';
import { baseHeaders } from '../../../libs/http/interceptors';

// 1. Custom Metrics (Trends for latency, Counters for business events)
const productListDuration = new Trend('product_list_duration_ms');
const productListErrors = new Counter('product_list_errors');

// 2. Test Data (SharedArray for memory efficiency)
const categories = new SharedArray('categories', function() {
  return JSON.parse(open('../../../data/test-data/categories.json'));
});

// 3. Scenario Config (use getConfig factory from libs/scenarios/profiles.ts)
export const options = getConfig('baseline', {
  scenarios: {
    browse_catalog: {
      executor: 'constant-arrival-rate',
      rate: Number(__ENV.K6_RPS) || 5,
      timeUnit: '1s',
      duration: __ENV.K6_DURATION || '5m',
      preAllocatedVUs: 10,
      maxVUs: 50,
      tags: { feature: 'products', kind: 'browse', uc: 'UC001' },
    },
  },
  thresholds: {
    'http_req_duration{feature:products}': ['p(95)<300'],
    'http_req_failed{feature:products}': ['rate<0.005'],
    'checks{uc:UC001}': ['rate>0.995'],
  },
});

const BASE_URL = __ENV.BASE_URL || 'https://dummyjson.com';

// 4. VU Code
export default function() {
  const res = http.get(
    `${BASE_URL}/products?limit=20&skip=${Math.floor(Math.random() * 80)}`,
    { headers: baseHeaders(), tags: { name: 'list_products' } }
  );
  
  productListDuration.add(res.timings.duration);
  
  check(res, {
    'status is 200': (r) => r.status === 200,
    'has products array': (r) => Array.isArray(r.json('products')),
  }, { uc: 'UC001', step: 'list' });
  
  sleep(1);
}
```

## üîë Critical Patterns

### Tagging Strategy (MANDATORY)
```javascript
tags: { 
  feature: 'products',  // Domain area (products/users/auth/carts)
  kind: 'browse',       // Operation type (browse/search/login/checkout)
  uc: 'UC001'           // Use case ID from docs/casos_de_uso/
}
```

### Thresholds (Quality Gates)
```javascript
thresholds: {
  'http_req_duration{feature:products}': ['p(95)<300'],      // Feature-specific latency
  'http_req_failed{feature:products}': ['rate<0.005'],       // Feature-specific error rate
  'checks{uc:UC001}': ['rate>0.995'],                        // Use case check success
}
```

### Remote Modules (ALWAYS VERSION-PINNED)
```typescript
import { randomItem } from 'https://jslib.k6.io/k6-utils/1.4.0/index.js';
import papaparse from 'https://jslib.k6.io/papaparse/5.1.1/index.js';
```

## üö´ Common Pitfalls to Avoid

1. **DON'T load data from `data/fulldummyjsondata/`** in tests ‚Üí use `data/test-data/` only
2. **DON'T use closed-model executors** (`shared-iterations`, `per-vu-iterations`)
3. **DON'T expect DummyJSON POST/PUT/DELETE to persist** ‚Üí they return fake responses
4. **DON'T forget `sleep(1)` between iterations** ‚Üí prevents unrealistic hammering
5. **DON'T use unversioned remote modules** ‚Üí always pin versions in jslib.k6.io URLs

## üîß Development Workflow

### Local Testing
```bash
# Run single test
k6 run tests/api/products/browse-catalog.test.ts

# With custom RPS/duration
K6_RPS=10 K6_DURATION=2m k6 run tests/api/products/browse-catalog.test.ts

# Type check (no execution)
npm run typecheck  # runs: tsc --noEmit
```

### CI/CD Quality Gates (auto-run on PR/main)
- **PR Smoke**: 30-60s, 1-2 RPS, loose thresholds (`.github/workflows/k6-pr-smoke.yml`)
- **Main Baseline**: 5-10min, 5-10 RPS, strict SLOs (`.github/workflows/k6-main-baseline.yml`)
- **On-Demand**: Stress/soak via `workflow_dispatch` (`.github/workflows/k6-on-demand.yml`)

### Data Generation
```bash
# Generate test data from application dumps
node data/test-data/generators/generate-users.ts \
  --source data/fulldummyjsondata/users.json \
  --output data/test-data/users.csv \
  --sample-size 100
```

## üìä Use Case Documentation

Every test MUST have a corresponding use case doc in `docs/casos_de_uso/UC00X-<name>.md`:

```markdown
# UC001 - Browse Products Catalog

## SLOs
| Metric | Threshold |
|--------|-----------|
| http_req_duration{p95} | < 300ms |
| http_req_failed | < 0.5% |
| checks | > 99.5% |

## Implementation
- File: `tests/api/products/browse-catalog.test.ts`
- Scenario: `configs/scenarios/baseline.yaml`
```

## üé® Code Style

- **File naming**: `<action>-<resource>.test.ts` (e.g., `browse-catalog.test.ts`, `search-products.test.ts`)
- **Imports order**: k6 built-ins ‚Üí k6 metrics ‚Üí k6 data ‚Üí remote modules ‚Üí local libs
- **Custom metrics naming**: `<feature>_<action>_<unit>` (e.g., `product_list_duration_ms`)
- **Check descriptions**: Human-readable strings (e.g., `'status is 200'` not `'200'`)

## üöÄ SLO Targets by Feature

| Feature | P95 Latency | Error Rate | Checks |
|---------|-------------|------------|--------|
| products | < 300ms | < 0.5% | > 99.5% |
| auth | < 400ms | < 1% | > 99% |
| search | < 600ms | < 1% | > 99% |
| carts | < 500ms | < 1% | > 99% |

## üìã Plano de Escrita dos Casos de Uso

### Fase 1: An√°lise e Levantamento ‚úÖ COMPLETA

**Objetivo**: Mapear todos os endpoints da API DummyJSON e categorizar por dom√≠nio

**Atividades**:
1. **Invent√°rio de Endpoints** ‚úÖ
   - Ler toda documenta√ß√£o em `docs/dummyJson/`
   - Listar todos os endpoints dispon√≠veis (GET, POST, PUT, DELETE)
   - Categorizar por dom√≠nio: Products, Auth, Users, Carts, Posts, Comments
   - Identificar depend√™ncias entre endpoints (ex: login antes de /auth/me)

2. **An√°lise de Perfis de Usu√°rio** ‚úÖ
   - Identificar personas t√≠picas de e-commerce (visitante, comprador, admin)
   - Mapear fluxos de neg√≥cio reais (navega√ß√£o, compra, administra√ß√£o)
   - Definir distribui√ß√£o de carga (% de cada perfil)

3. **Benchmarking de SLOs** ‚úÖ
   - Executar requests manuais para estabelecer baseline de lat√™ncia
   - Documentar tempos m√©dios por tipo de opera√ß√£o (read vs write)
   - Definir SLOs iniciais conservadores (refinar depois)

**Entreg√°veis**:
- ‚úÖ [`fase1-inventario-endpoints.csv`](../docs/casos_de_uso/fase1-inventario-endpoints.csv) - 38 endpoints catalogados
- ‚úÖ [`fase1-perfis-de-usuario.md`](../docs/casos_de_uso/fase1-perfis-de-usuario.md) - 3 personas com distribui√ß√£o 60/30/10
- ‚úÖ [`fase1-baseline-slos.md`](../docs/casos_de_uso/fase1-baseline-slos.md) - SLOs por feature (P95 < 300-600ms)

---

### Fase 2: Prioriza√ß√£o e Roadmap (Semana 2)

**Objetivo**: Definir ordem de implementa√ß√£o baseada em criticidade e complexidade

**Atividades**:
1. **Matriz de Prioriza√ß√£o**
   - Eixo X: Criticidade de neg√≥cio (core vs secund√°rio)
   - Eixo Y: Complexidade t√©cnica (simples vs complexo)
   - Classificar cada caso de uso identificado

2. **Defini√ß√£o de Fases**
   - Fase 0: Casos fundamentais (smoke test viability)
   - Fase 1: Autentica√ß√£o e controle de acesso
   - Fase 2: Opera√ß√µes CRUD principais
   - Fase 3: Jornadas compostas
   - Fase 4: Casos avan√ßados (resili√™ncia, consist√™ncia)

3. **Depend√™ncias e Pr√©-requisitos**
   - Mapear quais UCs dependem de outros (ex: Cart precisa de Auth)
   - Identificar dados de teste necess√°rios por UC
   - Planejar gera√ß√£o de massa de teste

**Entreg√°veis**:
- Matriz de prioriza√ß√£o visual
- Roadmap de implementa√ß√£o por sprint
- Mapa de depend√™ncias entre UCs

---

### Fase 3: Template e Padr√µes (Semana 3)

**Objetivo**: Criar templates reutiliz√°veis para documenta√ß√£o consistente

**Atividades**:
1. **Template de Caso de Uso**
   - Criar `docs/casos_de_uso/templates/use-case-template.md`
   - Definir se√ß√µes obrigat√≥rias vs opcionais
   - Incluir exemplos de preenchimento

2. **Conven√ß√µes de Nomenclatura**
   - Padr√£o de IDs: UC001, UC002, etc.
   - Padr√£o de nomes de arquivo: `UC00X-kebab-case-name.md`
   - Tags obrigat√≥rias: feature, uc, kind

3. **Estrutura de Fluxos**
   - Nota√ß√£o para descrever passos (numera√ß√£o, indenta√ß√£o)
   - Como documentar valida√ß√µes (checks)
   - Como especificar think times

**Entreg√°veis**:
- Template markdown completo em `docs/casos_de_uso/templates/`
- Guia de estilo para escrita de UCs
- Checklist de revis√£o de qualidade

---

### Fase 4: Escrita dos Casos de Uso (Semanas 4-7)

**Objetivo**: Documentar todos os casos de uso priorizados

**Sprint 1 (Semana 4) - Casos Fundamentais**:
- UC001: Browse Products Catalog
- UC002: Search & Filter Products
- **Meta**: 2 UCs completos, validados, com massa de teste identificada

**Sprint 2 (Semana 5) - Autentica√ß√£o**:
- UC003: User Login & Profile
- UC004: List Users (Admin)
- **Meta**: Fluxos de auth documentados, tokens mapeados

**Sprint 3 (Semana 6) - Opera√ß√µes Principais**:
- UC005: Cart Operations (Read)
- UC006: Cart Operations (Write - Simulated)
- **Meta**: CRUD completo, limita√ß√µes de simula√ß√£o documentadas

**Sprint 4 (Semana 7) - Jornadas**:
- UC007: User Journey (n√£o autenticado)
- UC008: User Journey (autenticado)
- UC009: Mixed Workload
- **Meta**: Cen√°rios compostos com think times realistas

**Atividades por UC**:
1. Descrever perfil de usu√°rio e objetivo de neg√≥cio
2. Listar endpoints envolvidos com m√©todos HTTP
3. Definir SLOs espec√≠ficos (baseado em baseline)
4. Detalhar fluxo passo a passo com valida√ß√µes
5. Especificar dados de teste necess√°rios
6. Documentar headers, payloads, query params
7. Identificar edge cases e cen√°rios de erro

**Entreg√°veis por Sprint**:
- X casos de uso documentados em markdown
- Massa de teste identificada (ainda n√£o gerada)
- Review de qualidade com checklist

---

### Fase 5: Valida√ß√£o e Refinamento (Semana 8)

**Objetivo**: Revisar e ajustar casos de uso antes da implementa√ß√£o

**Atividades**:
1. **Revis√£o por Pares**
   - Revisar cada UC com outro membro do time
   - Validar clareza e completude
   - Verificar ader√™ncia ao template

2. **Valida√ß√£o com Stakeholders**
   - Apresentar UCs para product owners
   - Confirmar que perfis de usu√°rio est√£o corretos
   - Ajustar SLOs baseado em expectativas de neg√≥cio

3. **Testes de Viabilidade**
   - Executar requests manuais para cada UC
   - Confirmar que endpoints existem e funcionam
   - Documentar particularidades descobertas

4. **Refinamento Final**
   - Ajustar SLOs baseado em testes manuais
   - Adicionar observa√ß√µes importantes
   - Atualizar depend√™ncias descobertas

**Entreg√°veis**:
- Todos os UCs revisados e aprovados
- Ata de valida√ß√£o com stakeholders
- Notas de viabilidade t√©cnica

---

### Fase 6: Handoff para Implementa√ß√£o (Semana 9)

**Objetivo**: Preparar documenta√ß√£o para time de implementa√ß√£o

**Atividades**:
1. **Organiza√ß√£o Final**
   - Numerar UCs em ordem de implementa√ß√£o
   - Criar √≠ndice em `docs/casos_de_uso/README.md`
   - Linkar UCs com endpoints da API docs

2. **Gera√ß√£o de Massa de Teste**
   - Criar geradores em `data/test-data/generators/`
   - Extrair amostras de `data/fulldummyjsondata/`
   - Versionar dados no Git

3. **Documenta√ß√£o de Suporte**
   - Criar guia de implementa√ß√£o
   - Documentar padr√µes de c√≥digo esperados
   - Preparar exemplos de testes

**Entreg√°veis**:
- README naveg√°vel de casos de uso
- Massa de teste gerada e versionada
- Guia de implementa√ß√£o para devs

---

### üìù Estrutura do Template de UC (Resumo)

```markdown
# UC00X - [Nome do Caso de Uso]

## üìã Descri√ß√£o
[Perfil de usu√°rio, objetivo, contexto de neg√≥cio]

## üîó Endpoints Envolvidos
[Lista de endpoints com m√©todo HTTP e SLO individual]

## üìä SLOs
[Tabela com m√©tricas, thresholds e rationale]

## üì¶ Dados de Teste
[Arquivos necess√°rios, volume, fonte, refresh strategy]

## üîÑ Fluxo Principal
[Passos numerados com requests, valida√ß√µes, think times]

## üîÄ Fluxos Alternativos
[Cen√°rios de erro, edge cases]

## ‚öôÔ∏è Implementa√ß√£o
[Onde ser√° implementado, configs, tags]

## üß™ Comandos de Teste
[Como executar localmente]

## üìà M√©tricas Customizadas
[Trends e Counters espec√≠ficos deste UC]

## ‚ö†Ô∏è Observa√ß√µes Importantes
[Limita√ß√µes, depend√™ncias, particularidades]
```

---

### ‚úÖ Checklist de Qualidade por UC

Antes de considerar um UC completo, verificar:

- [ ] Perfil de usu√°rio est√° claro e realista
- [ ] Todos os endpoints est√£o documentados com m√©todo HTTP
- [ ] SLOs est√£o definidos e justificados
- [ ] Fluxo principal est√° detalhado passo a passo
- [ ] Valida√ß√µes (checks) est√£o especificadas
- [ ] Dados de teste est√£o identificados (fonte + volume)
- [ ] Headers obrigat√≥rios est√£o documentados
- [ ] Think times est√£o especificados onde necess√°rio
- [ ] Edge cases e cen√°rios de erro est√£o mapeados
- [ ] Depend√™ncias de outros UCs est√£o listadas
- [ ] Limita√ß√µes da API (ex: fake POST) est√£o documentadas
- [ ] Arquivo nomeado corretamente: `UC00X-kebab-case.md`

---

### üéØ Ordem de Escrita Recomendada

1. **Come√ßar pelos mais simples**: Browse, Search (sem auth)
2. **Depois auth**: Login, Profile (funda√ß√£o para outros)
3. **CRUD principais**: Carts, Users (opera√ß√µes isoladas)
4. **Jornadas simples**: Combinar 2-3 UCs
5. **Jornadas complexas**: Autentica√ß√£o + m√∫ltiplas opera√ß√µes
6. **Casos avan√ßados**: Resili√™ncia, valida√ß√µes complexas

### üìä M√©tricas de Progresso

- **Sprint 1**: 2 UCs fundamentais (10% do total)
- **Sprint 2**: +2 UCs auth (30% do total)
- **Sprint 3**: +2 UCs CRUD (50% do total)
- **Sprint 4**: +3 UCs jornadas (80% do total)
- **Refinamento**: +2 UCs avan√ßados (100%)

---

## üìö Key References

- [k6 TypeScript Support](https://grafana.com/docs/k6/latest/using-k6/javascript-typescript-compatibility-mode/)
- [k6 Executors](https://grafana.com/docs/k6/latest/using-k6/scenarios/executors/)
- [DummyJSON API Docs](https://dummyjson.com/docs)
- [DummyJSON Products API](https://dummyjson.com/docs/products)
- [DummyJSON Auth API](https://dummyjson.com/docs/auth)
- [DummyJSON Carts API](https://dummyjson.com/docs/carts)
- [jslib.k6.io](https://jslib.k6.io/)
- Full PRD: `docs/planejamento/PRD.md`
